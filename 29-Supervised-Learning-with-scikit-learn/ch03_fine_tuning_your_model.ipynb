{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83d5eb4-b3de-47c7-be9a-b7244cf36cd3",
   "metadata": {},
   "source": [
    "# Chapter #3: Fine-Tuning Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bc5b4-d172-4a76-9bee-1ae1ebe81bb1",
   "metadata": {},
   "source": [
    "## 1. How good is your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b195a-87b3-4c65-86a6-7b78f6392771",
   "metadata": {},
   "source": [
    "**Classification metrics**\n",
    "> - Thinking back to **classification** problems, recall that **we can use accuracy, the fraction of correctly classified labels, to measure model performance**.\n",
    "> - However, **accuracy is not always a useful metric**.\n",
    "\n",
    "**Class imbalance**\n",
    "> - Consider a model for **predicting whether a bank transaction is fraudulent**, where **only 1% of transactions are actually fraudulent**.\n",
    "> - We could build a model that **classifies every transaction as legitimate**; this model would have an **accuracy of 99%!**\n",
    "> - However, **it does a terrible job of actually predicting fraud**, so it fails at its original purpose.\n",
    "> - The situation where one class is more frequent is called class **imbalance**.\n",
    "> - Here, the class of **legitimate transactions** contains way more instances** than the class of fraudulent transactions.\n",
    "> - This is a common situation in practice and requires a **different approach to assessing the model's performance**.\n",
    "\n",
    "**Confusion matrix for assessing classification performance**\n",
    "> - Given a **binary classifier**, such as our fraudulent transactions example, we can create a **2x2 matrix** that summarizes performance called a **confusion matrix**.\n",
    "> - Across the top are the **actual labels**, and down the side are the **predicted labels**.\n",
    "> - Given any model, we can fill in the confusion matrix according to its predictions:\n",
    ">> - The **true positives** are the number of fraudulent transactions correctly labeled,\n",
    ">> - The **true negatives** are the number of legitimate transactions correctly labeled,\n",
    ">> - The **false negatives** are the number of legitimate transactions incorrectly labeled,\n",
    ">> - And the **false positives** are the number of transactions incorrectly labeled as fraudulent.\n",
    "> - Usually, **the class of interest is called the positive class**.\n",
    ">> - As we aim to detect fraud, **the positive class is an illegitimate transaction**.\n",
    "> - **So, why is the confusion matrix important?**\n",
    ">> - Firstly, **we can retrieve accuracy**: it's the sum of true predictions divided by the total sum of the matrix,\n",
    ">> - Secondly, **there are other important metrics we can calculate from the confusion matrix**.\n",
    "\n",
    "<img style=\"margin-left: auto; margin-right: auto;\" src=\"./assets/ch03_01_how_good_is_your_model_img01.png\">\n",
    "\n",
    "**Precision**\n",
    "> - **Precision is the number of true positives divided by the sum of all positive predictions**.\n",
    "> - It is also called the **positive predictive value**.\n",
    "> - In our case, this is **the number of correctly labeled fraudulent transactions divided by the total number of transactions classified as fraudulent**.\n",
    "> - **High precision** means having a **lower false positive rate**.\n",
    "> - For our classifier, this translates to **fewer legitimate transactions being classified as fraudulent**.\n",
    "\n",
    "**Recall**\n",
    "> - **Recall is the number of true positives divided by the sum of true positives and false negatives**.\n",
    "> - This is also called **sensitivity**.\n",
    "> - **High recall reflects a lower false negative rate**.\n",
    "> - For our classifier, it means **predicting most fraudulent transactions correctly**.\n",
    "\n",
    "<img style=\"margin-left: auto; margin-right: auto;\" src=\"./assets/ch03_01_how_good_is_your_model_img02.png\">\n",
    "\n",
    "**F1 score**\n",
    "> - The **F1-score is the harmonic mean of precision and recall**.\n",
    "> - This metric **gives equal weight to precision and recall, therefore it factors in both the number of errors made by the model and the type of errors**.\n",
    "> - The F1 score **favors models with similar precision and recall**, and is a **useful metric if we are seeking a model which performs reasonably well across both metrics**.\n",
    "\n",
    "<img style=\"margin-left: auto; margin-right: auto;\" src=\"./assets/ch03_01_how_good_is_your_model_img03.png\">\n",
    "\n",
    "**Confusion matrix in scikit-learn**\n",
    "> - Using our churn dataset, to compute the confusion matrix, along with the metrics, we import **classification_report** and **confusion_matrix** from `sklearn.metrics`.\n",
    "> - We **instantiate** our classifier, **split** the data, **fit** the training data, and **predict** the labels of the test set.\n",
    "\n",
    "<img style=\"margin-left: auto; margin-right: auto;\" src=\"./assets/ch03_01_how_good_is_your_model_img04.png\">\n",
    "\n",
    "**Confusion matrix in scikit-learn**\n",
    "> - We pass the **test set labels** and the **predicted labels** to the **confusion matrix function**.\n",
    "> - We can see `1106` true negatives in the top left.\n",
    "\n",
    "<img style=\"margin-left: auto; margin-right: auto;\" src=\"./assets/ch03_01_how_good_is_your_model_img05.png\">\n",
    "\n",
    "**Classification report in scikit-learn**\n",
    "> - Passing the **same arguments** to **classification report** outputs **all the relevant metrics**.\n",
    "> - It includes **precision** and **recall** by class, `0.76` and `0.16` for the churn class respectively, which **highlights how poorly the model's recall is on the churn class**.\n",
    "> - **Support** represents the **number of instances for each class within the true labels**.\n",
    "\n",
    "<img style=\"margin-left: auto; margin-right: auto;\" src=\"./assets/ch03_01_how_good_is_your_model_img06.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da540ef-f627-4e23-a2ca-816d767575e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data-Science",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
